\chapter{Statistical Modeling of Solutions}\label{chapter:bayesian}

\section{Introduction}

The OverCode work began with the vision that a teacher would someday be able to look at a display summarizing hundreds or thousands of student solutions to the same problem and immediately see---and comment on---good and bad design decisions that students made. There will be some design decisions within each solution that are rare and some that are common, some that exemplify good programming practices and others that do not. These decisions might create inefficient solutions, reveal a student's fundamental misunderstanding, or use a feature of the language in a creative way.

Hundreds or thousands of solutions to the same introductory python programming prompt can share a lot of structure and behavior. Empirically, the number of different design decisions made by students can be relatively small, or at least distributed in such a way that the majority of students' decisions fall into a small number of categories, with the remaining students making much more unique design choices. 

However, the number of distinct student solutions grows exponentially with the number of locations in the program where students do not all make the same design decision. The OverCode canonicalization and stacking pipeline reduces the number of distinct solutions by collapsing dimensions that exist only because of differences in variable names or behaviorally equivalent statement orders. The pipeline helps reduce the number of student solutions a teacher needs to consider, but it does not fulfill the original vision: turning thousands of solutions into a small set of solution exemplars or facets of solutions that exemplify the underlying student design decisions.

Given the relatively small set of underlying design decisions, the common structures shared by many student solutions, the availability of feature vectors already computed by the OverCode pipeline, and earlier promising experimentation with Bayesian non-parametric methods as part of a collaboration with Been Kim \citet{kimthesis,kimtechreport}, Bayesian non-parametric methods may be key to fulfilling the original vision for OverCode.

\section{Empirical Qualities of Solution Clusters}

Clustering python solutions has a strong subjective component. In early pilot studies, python programming teachers given the same set of solutions partitioned the space of solutions in a variety of ways, producing different numbers and/or compositions of clusters. Since teachers found multiple reasonable clusterings, the pilot results can be explained by one or both of the following reasons: (1) Teachers have different internal clustering metrics. (2) Given a clustering metric, solutions can still belong to multiple clusters.

It is also reasonable to assume that, given additional solutions to the same programming problem, some solutions would fall into existing clusters and some would represent wholly new solution successes or failures. It is reasonable to assume there is not some true total number of clusters that one reaches if one sees enough data.

\section{Relevant Bayesian Models}

There are a variety of Bayesian methods that can handle one or more of the difficulties of clustering python solutions. The following methods are potentially good fits: 
\begin{itemize}
\item Latent Dirichlet Allocation (LDA) models solutions as "documents" with sets of "words," where each word belongs to one topic and each document can contain words from multiple topics.
\item Dirichlet Process Mixture Models (DPMMs) do not require the number of clusters to be set beforehand; the number of clusters solutions are assigned to can grow as the number of solutions grow. However, every data point belongs to one cluster.
\item Hierarchical Dirichlet Process Models (HDPs), like DPMMs, do not require a pre-set number of clusters. Unlike DPMMs, solutions do not belong to a single cluster. Like LDA, solutions contain features, and each feature belongs to a cluster. Solutions can contain features from multiple clusters. This method emulates LDA with no pre-set number of clusters. 
\item Models built on Indian Buffet Processes (IBPs) \citet{finaleMastersThesis} are like HDPs, but allow individual features can belong to multiple clusters.
\item Bayesian Case Models (BCM) \citet{kimthesis} learn a pre-set number of subspace clusters, where each cluster is represented by an example and a small set of features that play an "important role" in identifying that cluster \citet{kimnips}. This representation of the clusters has been designed to increase human interpretability of the results. This model has has an interactive version, iBCM \cite{kimbcminteractive}, where humans can directly modify the cluster example and important features that characterize a cluster.
\item Mind the Gap model (MGM) clusters data while also learning a "global set of distinguishable dimensions to assist with further data exploration and hypothesis generation"  \citet{mgmnips}.
\end{itemize}

\section{Model Choice}

Since (1) we do not know how many clusters will exist in the data ahead of time and (2) new solutions may introduce new clusters (there is no finite limit on the number of clusters as more solutions are received), models built on Dirichlet Processes are a good fit. 

Since solutions may contain multiple design decisions, it is preferable to be able to assign each of those design decisions separately, rather than assigning the entire solution to a cluster. HDPs are a good fit. If different design decisions share common features, IBPs may be an even better fit.

Since the output of LDA and its infinite cousin, the HDP, can be difficult to interpret \citet{ldainterpretation}, BCM or MGM's optimizations for interpretability may make them more appropriate choices.

Since teachers develop their own internal rubric for which differences matter and which do not, models which can incorporate and adjust based on feedback from the user, like iBCM, may be good model choices.

Each of these models have favorable properties, and some properties can be super-imposed, while others are mutually exclusive. There is also previously ignored factors like the difficulty of implementing an inference algorithm for the model and the scalability and time requirements of running the chosen inference algorithm on the chosen representation of the solutions.

\section{Feature Choice}

Once a collection of solutions to the same problem have been processed by the OverCode pipeline, individual solutions can be represented as sets of abstract variables, collections of canonicalized lines, and collections of templates.\footnote{Abstract variables, canonicalized lines, and templates are all described in Chapter \ref{chapter:overcode}.}

\section{Applying Models to Solution Collections}

\subsection{BCM}
[prior work]

\subsection{DPMM}

I will first implement a DPMM for 

\subsection{LDA}

\subsection{HDP}



\section{Predicting Rubric Labels}


