% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
%
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
%
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.

In a massive open online course (MOOC), a single programming exercise may yield thousands of student solutions that vary in many ways, some superficial and some fundamental. Understanding large-scale variation in programs is a hard but important problem.  For teachers, this variation can be a source of pedagogically valuable examples and expose corner cases not yet covered by autograding. For students, the variation in a large class means that other students may have struggled along a similar solution path, hit the same bugs, and can offer hints based on that earned expertise.

This thesis describes three systems that explore the value of solution variation in large-scale programming and simulated digital circuit classes. All three systems have been evaluated using data or live deployments in on-campus or edX courses with thousands of students. (1) OverCode visualizes thousands of programming solutions using static and dynamic analysis to cluster similar solutions. It lets teachers quickly develop a high-level view of student understanding and misconceptions and provide feedback that is relevant to many student solutions. (2) Foobaz clusters variables in student programs by their names and behavior so that teachers can give feedback on variable naming. Rather than requiring the teacher to comment on thousands of students individually, Foobaz generates personalized quizzes that help students evaluate their own names by comparing them with good and bad names from other students. (3) ClassOverflow collects and organizes solution hints indexed by the autograder test that failed or a performance characteristic like size or speed. It helps students reflect on their debugging or optimization process, generates hints that can help other students with the same problem, and could potentially bootstrap an intelligent tutor tailored to the problem. These systems demonstrate how clustering and visualizing student solutions helps teachers and students provide types of one-on-one design feedback at scale that was previously only possible in a small classroom or one-on-one tutoring. The design choices around which these scaled-up forms of feedback rest can be curated by teachers directly from trends and outliers within the students' own solutions. The feedback generated by both teachers and students can be re-used by future students who attempt the same programming or hardware design problem.

\begin{comment}

helps teachers respond to class-population-level trends and outliers in student designs, and curate pedagogically examples that can then These systems demonstrate how, 

Foobaz and ClassOverflow demonstrate how practices that previously could only occur in one-on-one interactions or wtihin small classrooms can be scaled up to serve thousands of current and future students. User testing of OverCode and its extensions demonstrate how visualizing and clustering large collections of code can help teachers gain insight into and take advantage of the space of student-generated solutions in ways that were not possible in smaller classrooms.

Massive programming courses produce a massive collection of student solutions for each programming exercise. The solutions to any particular problem vary along many dimensions, including bugs, naming, syntax, and semantics. The distribution of solutions along these dimensions reflect students' prior knowledge, the teacher's course curriculum and explanations so far, and misconceptions common to all programming students.

Personal tutors can respond to an individual tutee's solution, and only draw on their personal recollection of previously observed solution variation. Teachers teaching larger groups of students can directly observe that a significant fraction of students are struggling with a particular concept or implementation, and respond appropriately with rapid contextual feedback. They can also pick out particular student solutions as examples to illustrate different concepts or ways of solving a problem, rather than solely relying on their own creativity to composing these examples from scratch. 

When scaling up to massive programming courses, it becomes painful or prohibitively exhausting to engage with student solutions this way. It is also an opportunity: teachers have access to a comparatively dense sampling of the distribution over student solution bugs, naming, syntax, and semantics. 

The systems in this thesis help teachers take advantage of the massive collections of solutions, enabling either (1) the same teaching practices that were previously only tractable in smaller courses or (2) new practices that are only possible when a massive collection of student solutions are available. The common empowering mechanism is clustering and visualizing solution variation for human understanding. Using these systems, teachers can gain insights into student design choices, detect autograder failures, process solutions that deserve partial credit, use targeted learnersourcing to collect hints for other students, and give personalized style feedback at scale.
\end{comment}
