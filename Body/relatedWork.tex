\chapter{Related Work}\label{chapter:relatedwork}



Systems that help students in massive programming courses may build on work from program analysis, program synthesis, crowd workflows, user-interface design, machine learning, intelligent tutoring systems, natural language processing, data mining, and learning science. First, I present prior work and theories of how people learn that later inspired key design decisions or suggest that the systems in this thesis are beneficial in a learning environment. I then clarify how this thesis work is novel while describing related work that achieves similar goals or uses similar methods. %\todo{add references to my own work, to illustrate relevance}

Effective tutors often have characteristics described by Lepper and Wolverton's INSPIRE model: superior domain and pedagogical content knowledge, nurturing relationships with students, progressive content delivery, Socratic styles that prompt students to explain and generalize, and feedback on solutions, not students~\cite{wood2012role}. Turns et al. \cite{asee} argue that the absence of reflection in traditional engineering education is a significant shortcoming. This thesis contributes systems designed to address that shortcoming.\todo{rewrite this paragraph}

\section{Scalable teaching principles}

%Learning how to write programs, or at least how to “think computationally,'' has gained national attention in the last year. Last September, New York City Mayor Bill de Blasio announced that all public schools in NYC will be required to offer computer science to all students by 2025. In January, the White House released its “Computer Science For All'' initiative, “offering every student the hands-on computer science and math classes that make them job-ready on day one'' (President Obama, 2016 State of the Union Address).

%Schools like MIT, Stanford, Berkeley and the University of Washington have expanded to accommodate hundreds or thousands of students in a single programming class. Many more complete exercises on tutorial websites like Codecademy, Kahn Academy, and Code School or enroll in coding schools and bootcamps like General Assembly and Hackbright Academy. One-on-one tutoring is considered a gold standard in education, and the teacher-to-student ratio is only getting worse.

%As we develop new tools, techniques, and curriculum to serve more students, it is important to be grounded in, or at least knowledgeable of, the work that researchers in educational psychology have been doing for decades: teasing apart what it takes to learn something efficiently and well.

%There are many factors inside and outside the classroom that have significant effects on learning. 

%I will focus on a few techniques and theories from the learning sciences that a human may be able to execute better with a computer than without. 

%There is an entire literature on developing sustainable communities of practice that foster student development and mastery, much the way a traditional judo dojo operates. There is also great work on how identity formation can help budding experts persist and thrive in a learning environment. For a high level treatment of both those literatures as they pertain to engineering education, read this brief handbook for practitioners: Education theories on learning: an informal guide for the engineering education scholar.

%However, I will focus on a few theories and concepts that specifically help teachers develop more effective presentations of information and exercises for practice. This short-list of ideas, techniques, and benchmarks from educational psychology have guided my own development of tools for teachers teaching hundreds or thousands of programming students at once.

%\subsection{Tutoring}

%Tutoring has been held up as a gold standard in education since 1984 when Bloom published a collection of his lab's work demonstrating tutoring's efficacy relative to other experimental and conventional methods at the time~\cite{bloom}. For example, his lab found that, after 11 sessions of instruction in probability or cartography, elementary and middle school students who received tutoring\todo{RobTODO: find more precise word} in groups of one to three were, on average, two standard deviations better than their counterparts in a conventional 30-person classroom. Given the expense of scaling up one-on-one tutoring, Bloom challenged the academic community to find a method of group instruction that was just as good, or better. That challenge still stands as a benchmark that modern systems and techniques can compare against. While I do not measure the relative learning gains associated with the systems presented in this thesis, I do relate key design decisions to the tutoring best practices that are able to be scaled up or automated, such as prompting for self-explanations.% evidence from the learning sciences The focus on this thesis is on developing systems that scale up what good tutors and teachers do.

\subsection{Self-Explanation}

Self-explanations are generated by the student for themselves. They are a form of reflection, which is a critical method for triggering the transformation from conflict and doubt into clarity and coherence \cite{dewey1933}. Students' self-explanations foster the integration of new knowledge, while some tutors' explanations may be insufficient or flawed, due to the curse of knowledge~\cite{birch2007curse}. Effective tutors encourage self-explanation by prompting students with questions like {\it Why?} and {\it How?}~\cite{selfexplanation}. Students of tutors who fostered self-explanations had learning gains similar to those whose tutors provided their own explanations and feedback\cite{chi2001learning}. 

%One insight stands out: the value of self-explanation for fostering student learning.

%\subsubsection{Self-Explanation}

%Reflection is also a critical method for triggering the transformation from conflict and doubt into clarity and coherence \cite{dewey1933}. Turning that reflection into a self-explanation further improves understanding \cite{selfexplanation}.

\begin{comment}
Turns et al. \cite{asee} argue that the absence of reflection in traditional engineering education is a significant shortcoming.
\end{comment}

%\subsubsection{Curse of Knowledge}

%While explanations generated by tutees may help them integrate new knowledge, explanations generated by experts, i.e., the tutor, may be insufficent or flawed, due to the curse of knowledge \cite{birch2007curse}.

%Reflection is a critical method for triggering the transformation from conflict and doubt into clarity and coherence \cite{dewey1933}. Turning that reflection into a self-explanation further improves understanding \cite{selfexplanation}. According to Turns et al. \cite{asee}, the absence of reflection in traditional engineering education is a significant shortcoming. 

\subsection{Deliberate Practice and Rapid Feedback}

%Ericsson is one of the foremost experts on how learners can efficiently acquire domain-specific knowledge and skills, like those necessary for becoming an effective programmer. 

Deliberate practice is generally accepted to be goal-directed, effortful, repetitive, accompanied by rapid feedback, and only sustained as long as the learner can be fully concentrated on the task, i.e., no more than a few hours~\cite{Gobet2012}. For example, rather than just playing pickup basketball games in the neighborhood, an aspiring professional player might design specific drills to work on his/her weaknesses. Teachers help facilitate deliberate practice, because they can design appropriate exercises and provide feedback until the student can differentiate between good and bad performance and provide that feedback to themselves. Recent work incorporating deliberate practice in large classrooms has demonstrated great benefits. A recent study of undergraduate physics classrooms found that, with deliberate practice as a base of the instructional design, improvements can approach and exceed Bloom's 2-sigma threshold~\cite{Deslauriers862}. Another recent study confirms the value of rapid feedback in foundation engineering classrooms \cite{ieeeRapidFeedback}. 

This thesis contributes systems designed to provide feedback on some aspects of programming tasks where it was not feasible to do so before, such as Foobaz, which stimulates reflection and provides feedback on one of the most basic forms of program readability: variable names.\todo{ROBTODO: REWRITE AND FLESH OUT INTO ITS OWN PARAGRAPH.}

\subsection{Zone of Proximal Development and Scaffolding}

The concept of the zone of proximal development (ZPD) was first introduced in the mid 1920's by the Soviet pyschologist Lev Vygotsky. It refers to the gap between what a learner can do without help and what a learner cannot yet do, no matter how much help they are given. It is implied that an object of learning strictly outside the ZPD is either too easy or too hard, and little or no learning will occur.

Wood et al.~\cite{woodscaffolding} introduced a complementary process called scaffolding. Scaffolding enables a learner to solve problems or achieve goals that would ordinarily be beyond their grasp because the teacher controls the aspects of the task that are initially outside the learner's abilities. Recent work suggests that the maximum learning gains come from giving students the hardest possible tasks they are able, with the assistance of scaffolding, to complete~\cite{zpd14}. Formative feedback~\cite{formative} can be helpful as part of the scaffolding. It should non-evaluative, supportive, timely, and specific. It usually arrives as a response to a student's action, e.g., a hint, an explanation, or a verification based on the student answer. Based on this research, one of the components in this thesis, the comparison workflow, identifies where a student solution is on the spectrum of optimality and prompts the student to reflect on the {\it next most optimal} solution.

\subsection{The Role of Strategic Variation in Examples}

Studies of tutors and their students help us identify characteristics and styles of interaction that help explain the effectiveness of tutoring. Some of these can be successfully deployed in large classrooms. However, the way we frame content can also have large effects on how students understand, generalize, and transfer their learning to new contexts.

Concrete examples of an object of learning--like how to apply an appropriate statistical test in a statistics word problem--vary in ways that are superficial, e.g., irrelevant, and fundamental, e.g., relevant. In the language of educational psychologists, these are often called surface and structural features ~\cite{quilicimayer}. A simple compare and contrast exercise when solving equations~\cite{rittle2007does} or examining case studies in negotiation~\cite{loewenstein2003analogical} can bring this variation to the fore, and yield learning benefits.

Learning in the presence of variation in these features helps learners generalize and transfer their knowledge to new situations, such as better transfer of geometric problem solving skills~\cite{workedexamplesvariability,Variabilityofpractice}. Several educational models, e.g., Variation Theory and the 4C/ID Model~\cite{van2002blueprints}, build on the value of variability by suggesting specific ways for how it should be deployed in the classroom. The three components in this thesis, OverCode, Foobaz, and the targeted learnersourcing workflows, are all designed to make the natural variability present in student solutions useful to teachers and students, in accordance with recommendations from the educational theories that follow.

\subsubsection{Analogical Learning}

Analogies are central to human cognition. They can help learners understand and transfer knowledge and skills to new situations. Analogical learning is at play both when learners have a base of knowledge that they bring with them to a novel target and when they compare two partially understood situations that can illuminate each other, serving as both a source and recipient of information~\cite{kurtz01learning,loewenstein2003analogical}. However, in order to reap the full benefits of analogical learning, learners must engage deeply. Reading two cases, serially, in a session is not enough. Learners will not necessarily make the necessary connections unless there are explicit instructions to compare~\cite{loewenstein2003analogical,catrambone1989overcoming}. Kolodner~\cite{Kolodner} suggests creating software tools that align examples to facilitate analogical learning. This thesis is, in part, a response this suggestion.

Novices may become confused if asked to compare their solution to a fellow student's solution. This is not necessarily bad for learning outcomes. Piaget theorized that cognitive disequilibrium, experienced as confusion, could trigger learning due to the creation or restructuring of knowledge schema~\cite{disequilibrium}. D'Mello et al. maintain that confusion can be productive, as long as it is both appropriately injected and resolved~\cite{productiveconfusion}. Similarly, reflecting on a peer's conceptual development or alternative solution may bring about cognitive conflict that prompts reevaluation of the student's own beliefs and understanding \cite{kavanagh}. The comparison workflow component of this thesis is designed to stimulate this kind of productive confusion, comparison, and resolution through self-explanation. \todo{add Towards Providing Feedback to Students in Absence of Formalized Domain Models by Sebastian Gross, Bassam Mokbel, Barbara Hammer, Niels Pinkwart}

Analogical learning can be very difficult. For example, the structural features may be aligned between a base and the new target situation, but large differences in surface features will hurt the learner's ability to see any connection~\cite{Kurtz}. This may be explained by how human memory works. For novices, the most reliable form of retrieval is based on surface similarity, not deep analogical similarity. Experts can more easily retrieve situations that are structurally similar and therefore more relevant for a new situation at hand~\cite{Loewenstein}.

Variation Theory, discussed next, is specifically designed to help students more deeply appreciate structural features, which may help them transfer their learning to new situations instead of feeling lost, confused by superficial differences.

\subsubsection{Variation Theory: Preventing Human Overfitting}

This aphorism captures the ideas at the heart of Variation Theory (VT) well: {\it He cannot, England know, who knows England only.} VT is concerned with the way in which students are taught from concrete examples. It is relatively new and still being investigated for its usefulness in a broad range of disciplines, including mathematics and computer science. 

VT asserts that human learning can suffer from overfitting for some of the same reasons that machine learning algorithms do. Overfitting is a term I am borrowing from the machine learning community. If a machine learning algorithm selects the the color of the sky as the key difference between photos of cats and dogs (which is obviously unrelated to distinguishing between photos of cats and dogs), then a possible explanation is that the algorithm was trained on photos with insufficient or biased variation. If all the cat photos it ever saw were taken on a cloudy day, and all the dog photos it ever saw were taken on a sunny day, could you blame this naive program for latching on to this obvious differentiator of housepet species? Humans can make the same inferential mistake when not exposed to a sufficient variety of examples of an object of learning. VT catalogues a hierarchy of patterns of variation designed to immunize the learner to this kind of mistake.

More abstractly, VT is built on the understanding that learning is not possible without being able to discern what the object of learning is~\cite{marton1997learning}. Discernment is not possible without experiencing variation in the object of learning and the world in which its situated~\cite{marton2004classroom}. This variation is described in terms of aspects and features. An aspect refers to a dimension of variation, and a feature is a value of that dimension of variation~\cite{ling2012variation}. Some features are irrelevant, while critical features collectively define the object of learning.

For a more concrete discussion of variation, consider the following examples:
\begin{enumerate}
\item The phrase {\it a heavy object} might not make sense to the reader unless they have interacted with objects of various weights.
\item Consider a child who recently learned how to add numbers, but always starts with the larger number: 2+1=3, 4+2=6, etc. Asking the child to add the numbers in the opposite order, smaller number first, and verify that the result is the same introduces the commutative feature of addition.
\item No matter how wildly a cup diverges from a prototypical example of a tea cup, if it does not have the critical feature of being able to hold something, it is not a cup.
\end{enumerate}

Marton et al.~\cite{marton1997learning} identify four patterns of variation: contrast, separation, generalization, and fusion. If a child is learning the concept of {\it three}, then contrast refers to being introduced to three apples, as well as a pair of apples, or a dozen. Generalization refers to being introduced to different groups of three: three apples, three dogs, three beaches, and three languages. This clarifies that it is not the apples that give {\it three} its meaning. Separation refers to a pattern of examples that helps the learner separate a dimension of variation from other dimensions of variation. A child could be introduced to a litter of nearly identical puppies that only differ in coat color, for example. Fusion is the final pattern of variation, where the learner is exposed to examples that vary along all the dimensions of variation at once, since this is most commonly encountered in the real world. These patterns of variation are intended to reveal which aspects of a concept or phenomenon are superficial and irrelevant and which are innate and critical to its definition.

VT is a framework that has guided teaching materials and been used as an analytic framework in a variety of contexts, including lessons on critical reading~\cite{noble1998contents}, vocabulary learning~\cite{doi:10.1108/IJLLS-10-2014-0038}, the color of light~\cite{Ling2006}, mathematics~\cite{Pythagoras233}, chemical engineering~\cite{C2RP20145C}, Laplace transforms~\cite{carstensen2004laplace}, supply and demand~\cite{marton2006some} and computing education~\cite{suhonen2007applications}. It has been the subject of a government-funded three-year longitudinal study in Hong Kong, with promising results~\cite{lo2005each}. 

In this thesis, Overcode and Foobaz are explicitly designed to discover and make human-interpretable the variation naturally present in student solutions. All the systems in this thesis demonstrate ways in which extracted variation, in solutions or errors, can be used in massive classes. In the next section, I describe efforts to extract, answer questions about, or otherwise make usable the natural variation in corpuses of webpages, apps, and code that do not all have the same purpose. In the final section, I describe similar efforts specifically for corpuses of student solutions to the same programming exercise.


\section{Exploring and Mining Variation in the Wild}

One important aspect of both engineering and design is that there are multiple ways to solve the same problem. There are many means to an end, and the optimality of each solution may be context-dependent. Sub-optimality can have high personal, safety, and economic costs. For example, in the software industry, maintenance dominates the cost of producing software~\cite{fox2013engineering}. Poorly designed code may be the culprit because it requires significantly more maintenance.

\begin{comment}
There’s no silver bullet for becoming a great engineer. However, best practices will probably always include analyzing others’ designs, highlighting their successes and failures, as well as composing one’s own designs and receiving critiques from others.

This method of training is particularly exciting today, when there is such a diversity of design examples available online. Thousands of apps are available for download from app stores. Kaggle maching learning competition submissions are not public by default, but are sometimes released by authors and collected for the benefit of others. Github and Bitbucket host millions of repositories, many of them public and searchable.
\end{comment}

If solutions can be indexed in useful ways, they can be mined to ask basic questions like,
\begin{enumerate}
\item    In the face of a common design choice, what do people most commonly pick? Has this changed over time?
\item    What are popular design alternatives?
\item    What are examples of design fails that should be learned from and never repeated?
\item    What are examples of design innovations that are clearly head-and-shoulders above the rest?
\end{enumerate}
The answers to these questions could fuel instruction that complies with Variation Theory's recommendations. With OverCode, Foobaz, and the targeted learnersourcing workflows, I try to answer the same questions for students tackling the same programming challenge.

\subsection{Webpages}
Ritchie et al.~\cite{ritchie2011d} describe a user interface for finding helpful, i.e., relevant or inspiring, design examples from a curated database of web pages. Their work is intended to support designers who like to refer to or adapt previous designs for their own purposes. Traditional search engines only index the content of web pages. Their system indexes web pages’ design style by automatically extracting global stylistic and structural features from each page. Instead of manual browsing, users can search and filter a gallery of design-indexed pages. Users can provide an example design in order to find similar and dissimilar designs, as well as high-level style terms like ``minimal.''

Kumar et al.~\cite{webzeitgeist} defined {\it design mining} as ``using knowledge discovery techniques to understand design demographics, automate design curation, and support data-driven design tools.'' Their work goes beyond searching and filtering a gallery of hundreds of curated webpages. Their Webzeitgeist design mining platform allows users to query a repository of hundreds of thousands of web pages based on the properties of their Document Object Model (DOM) tree and the look of the rendered page. A 1679-dimensional vector of descriptive features are computed for each DOM node in each page.

Webzeitgeist enables users to ask and answer some of those originally highlighted questions, with respect to this large web page repository:
\begin{enumerate}
\item    What are all the distinct cursors?
\item    What are the most popular colors for text?
\item    How many DOM tree nodes does a typical page have? How deep is a typical DOM tree?
\item    What is the distribution of aspect ratios for images?
\item    What are the spatial distributions for common HTML tags?
\item    How do web page designers use the HTML canvas element?
\end{enumerate}

To dig into examples of a particular design choice, users can, for example, query for all pages with very wide images. The result is a set of horizontally scrolling pages. Alternatively, users can query for webpages that have a particular layout, like a large header, a navigational element at the top of the page, and a text node containing greater than some threshold of words, in order to see all the examples of pages that fit those layout specifications. Specific combinations of page features can imply high-level designs as well so, with careful query construction, users can query for high-level ideas. For example, querying for pages with a centered HTML input element AND low visual complexity retrieves many examples that look like the front pages of search engines.

\subsection{Android Apps}

Shirazi et al.~\cite{Shirazi} and Alharbi and Yeh~\cite{Alharbi} describe automated processes for taking apart and analyzing Android app code as well as empirical analyses of corpuses of Android Apps available on the Google Play app store. Shirazi et al. analyzed the 400 most popular free Android applications, while Alharbi and Yeh tracked over 24,000 Android apps over a period of 18 months. Alharbi and Yeh caputured each update within their collection window, as well. They decompiled apps into code from which UI design and behavior could be inferred, e.g., XML and click handlers, and tracked changes across versions of the same app. Both papers analysed population-level characteristics of their corpuses, answering questions like:
\begin{itemize}
\item     What is the distribution of layout design patterns, among the seven standard Android layout containers?
\item     What are the most common design patterns for navigation, e.g., tab layout and horizontal paging? Have any apps switched from one pattern to another?
\item     How quickly are newly introduced design patterns adopted?
\item     What are the most frequent interface elements? And combinations of interface elements? How many applications does that combination cover?
\end{itemize}

\subsection{Open-Source Code Repositories}

Ideally, code is not just correct, it is simple, readable, and ready for the inevitable need for future changes~\cite{peters2010zen,6005notes}. How can we help students reach this level of programming composition zen? How can we learn from others' code, even after we become competent, or even an expert, at the art of programming?

For the same reason we look at patterns in design across web pages and mobile apps, we can look at the design choices already made by humans who share their programs. Rather than using web crawlers or app stores, we can process millions of public repositories hosted online. What can we learn about good and bad code design decisions from these collections?

These code analysis techniques that follow are most closely related to those developed for OverCode and Foobaz. OverCode and Foobaz's pipelines are optimized for user interfaces that help users answer design mining questions about their students' compositions and put the code front and center. The advantages and disadvantages of the OverCode and Foobaz pipelines with respect to the systems designed in this section will be discussion in later chapters after the techniques developed in this thesis are fully explained.

\subsubsection{Regularity In Code}

Several papers make similar observations, arguments, and emperical validation of the regularity that can be found in code. Hindle et al.~\cite{Hindle2012} were motivated by the assertion that human-produced natural language and human-produced program language may both be "complex and admit a great wealth of expression, but what people write ... is largely regular and predictable." The authors argue that the assertion may be even more true for code than for natural language. Allamanis and Sutton~\cite{allamanis2014mining} observe that there are syntactic fragments, i.e., idioms, that serve a single semantic purpose and recur frequently across software projects. Fast et al.~\cite{codex} observe that poorly written code is often syntactically different from well written code, with the caveat that not all syntactically divergent code is bad.

\subsubsection{Mining Idioms}

Idiomatic code is written in a manner that experienced programmers perceive as normal or natural. Idioms are roughly equivalent to mental chunks, i.e., the memory units characterized by George Miller~\cite{chunking}. I will borrow an example from Allamanis and Sutton~\cite{allamanis2014mining}

\begin{itemize}
\item \texttt{for (int i=0;i<n;i++) ...} is a common idiom for looping in Java.
\item do-while and recursive looping strategies are not.
\end{itemize}

An experienced Java programmer will be able to understand the code whether it's idiomatic or not, but it may take longer. They may even be distracted by questions, e.g., {\it Why did the author make this choice?}

Fast et al.~\cite{codex} break the definition of idioms into two levels. An example of a high-level idiom is code that initializes a nested hash. An example of a low-level idiom is code that returns the result of an addition operation. Some languages support a variety of different, equally good ways to do the same thing. Others encourage a single, idiomatic way to achieve each task.

Idioms can and do recur throughout distinct projects and domains (unlike repeated code nearly verbatim, i.e., clones) and commonly involve syntactic sugar (unlike API patterns). In general, clone detectors look for the largest repeated code fragments and API mining algorithms look for frequently used sequences of API calls. Idiom mining is distinct because idioms have syntactic structure and are often wrapped around or interleaved with context-dependent blocks of code, like the block of statements within an the idiomatic for loop in the previous paragraph.

There are enough idioms for some languages that they have lengthy, highly bookmarked and shared online guides. StackOverflow has many questions asked and answered about the appropriate language or library-specific idioms for particular, common tasks. It is difficult for expert users of each language or library to catalogue all the idioms. It is much more practical to simply look at how programmers are using the language or library and extract idioms from the data.

Hindle et al.~\cite{Hindle2012} used statistical language models from natural language processing to identify idiom-like patterns in Java code. They found that n-gram language models built by analyzing corpuses captured a high level of project- and domain-specific local regularity in programs. Local regularities are valuable for statistical machine translation of natural language. They may prove useful in analogous tasks for software as well. For example, the authors trained and tested an n-gram model token suggestion engine that looks at the previous two tokens already entered into the text buffer and predicts the next one the programmer might type.

Allamanis and Sutton~\cite{allamanis2014mining} automatically mine idioms from a corpus of idiomatic code using nonparametric Bayesian tree substitution grammars. The mined idioms correspond to important programming concepts, e.g., object creation, exception handling, and resource management, and are, as expected, often library-specific. They found that 67\% of the idioms mined from one set of open source projects were also found in code snippets posted on StackOverflow.

Fast et al.~\cite{codex} computed statistics about the abstract syntax trees (ASTs) of three million lines of popular open source code in the 100 most popular Ruby projects hosted on Github. AST nodes are normalized, and all identical normalized nodes are collapsed into a single database entry. The unparsed code snippets that correspond to each normalized node are saved. Codex normalizes these snippets by renaming variable identifiers, strings, symbols, and numbers to \texttt{var0}, \texttt{var1}, \texttt{var2}, \texttt{str0}, \texttt{str1}, etc. Note that this fails when primitives, like specific strings and numbers, are vital to interpreting the purpose of the statement.

The resulting system, Codex, can warn programmers when they chain or compose functions, place a method call in a block, or pass an argument to a function that is infrequently seen in the corpus. It is fast enough to run in the background of an IDE, highlighting problem statements and annotating them with messages like, ``We have seen the function split 30,000 times and strip 20,000 times, but we've never seen them chained together.'' Codex can be queried for nodes by code complexity; type, i.e., function call; frequency of occurrence across files and projects; and containment of particular strings.

\subsubsection{Mining Larger Patterns in Code}

In the code of working applications, Ammons et al.~\cite{ammons2002mining} observed that common behavior is often correct. Based on that observation, they use probabilistic learning from program execution traces to infer the program’s formal correctness specifications. Inferring formal specifications for programs is valuable because programmers have historically been reluctant to write them. During program execution, the authors summarize frequent patterns as state machines that can be inspected by the programmer. As a result, the authors identified correct protocols and some previously unknown bugs.

Buse and Weimer~\cite{buse2012synthesizing} go beyond idioms to mining API useage. Based on a corpus of Java code, they find examples that reference a target class, symbolically execute it to compute intraprocedural path predicates while recording all subexpression values, identify expressions that correspond to one use of the class, capture the order of method calls in those concrete examples, then use K-mediods to cluster these extracted concrete use examples with a custom formal parameterized distance metric that penalizes for differences in method ordering and type information. Concrete use examples within the same cluster are merged into abstract uses represented as graphs with edge weights that correspond to counts of how many times node X happens before node Y. Finally, they have a synthesis method to express these abstract use graphs in a human-readable form, i.e., representative, well-formed, and well-typed Java code fragments.

\subsubsection{Mining Names}

Without modifying execution, names can express to the human reader the type and purpose of an object, as well as suggest the kinds of operators used to manipulate it~\cite{jones2008operand}. Perhaps as a direct result, variable names can exhibit some of the same regularity exhibited by code, in general. H{{\o{}}}st and {{\O{}}}stvold~\cite{host2008java} go so far as to call method names a restricted natural language they dubbed Programmer English.

H{{\o{}}}st and {{\O{}}}stvold~\cite{host2008java} ran an analysis pipeline on a corpus of Java that performs semantic analysis on methods and grammatical analysis on method names. It generates a data-driven phrasebook that Java programmers can use when naming methods. In a second publication~\cite{host2009debugging}, they formally defined and then automatically identified method naming bugs in code, i.e., giving a method a name that incorrectly implies what the method takes as an argument or does with an argument.

They did this by identifying prevalent naming patterns, e.g., contains-*, which occur in over half of the applications in the corpus and match at least 100 method instances. They also determined and cataloged the attributes of each method body, such as whether it read or wrote fields, created new objects, or threw exceptions. If almost all the methods whose names match a particular pattern, e.g., contains-*, have an attribute or do not have some other attribute, it automatically determined to be an implementation rule that all names in the corpus should follow. On a large corpus of Java projects, this analysis pipeline found a variety of naming bugs.

Fast et al.'s Codex~\cite{codex} produced similar results. By keeping track of variable names in variable assignment statements, it can warn programmers when their variable name violates statistically-established naming conventions, such as the (probably confusing) naming of a Hash object ``array.'' 

Foobaz can go beyond Fast et al.~\cite{codex} and H{{\o{}}}st and {{\O{}}}stvold's~\cite{host2008java,host2009debugging} work in providing feedback on variable names because all solutions are known to address the same programming exercise.

%\section{Variation and Commonality Across Student Solutions}
\section{Exploring and Mining Variation in Student Solutions}

The work in this thesis was composed during an eruption of interest in teaching massive numbers of people through online exercises and courses. Computer-based, scalable teaching environments, such as Intelligent Tutoring Systems, were not new, and neither was putting coursework up online. For example, MIT OpenCourseware opened its virtual doors to provide free access to static MIT course materials nearly 15 years ago, in 2002. However, it was not until 2011, that the first full university level course was made available to the public, complete with college-level programming exercises. Soon after, several organizations, e.g., edX, Coursera, and Udacity, started up to provide similar experiences for a range of university courses. I was acutely aware of edX because it was first organized from within the same basement lab of Stata in which I was spending my days and nights helping students complete assignments for Computation Structures.

%Andrew Ng at Stanford opened up his machine learning course infrastructure to the public, so that they could ''take'' his class--including its web-based programming exercises.

These organizations were soon sitting on top of piles of student activity and responses that could be studied by researchers. It became possible to formulate and answer new questions. The ACM Learning at Scale conference was started to give these a dedicated venue to talk about this new work. Multiple research groups began working on making sense and making use of this data to enhance learning outcomes and experiences. The work in this section is relevant to processing large corpuses of student solutions to the same programming exercise. %What set this work apart from previous research efforts was the idea that, with additional students %Early posters and workshop papers reporting interesting research questions and approaches with only preliminary results inspired several series of papers by research groups working in parallel.

The common purpose of the code in the corpus almost certainly contributes to the regularity already found in code from large corpuses of open source projects. However, this regularity from a common purpose may be more than counter-balanced by the fact that the code is generated by novices who are still learning how to program and are not yet well-versed in the common idioms and programming constructs. 

Unlike corpuses of open source projects, it is often not difficult to go beyond static analysis and perform dynamic analyses while executing each solution in the corpus. A set of teacher-designed tests of expected behavior may already exist, some available to students while developing their solutions, some hidden from students' view, and some generated on the fly, through fuzz testing \cite{fuzztesting}. %As a result, these solutions can be analyzed on both their syntactic and behavioral qualities.

Also unlike corpuses of open source code, the solutions in large corpuses of student code often require feedback, so that the author can learn. Automated feedback on solutions to programming exercises is still an area of active research. For example, assigning grades based solely on the number of teacher-designed tests the code passes may not capture what teachers care to grade on. A single error in a solution can cause a near-perfect solution to fail all test cases. A solution can perform perfectly on test cases but be poorly written or violate instructions, e.g., use the wrong algorithm to achieve the right results. The test cases themselves may be poorly designed. For these reasons, the teaching staff of 6.0001 at MIT review every exam solution from each student by hand.

When teachers have hundreds or thousands of students, it can be challenging to provide feedback to each solution quickly and consistently by hand. Design mining techniques and interfaces can help teachers explore and understand the whole space of solutions as well as distribute feedback to specific subsets of solutions, or subsets within solutions. It could be an important tool to assist in the sometimes difficult, manual task of identifying pedagogically valuable examples for illuminating a concept or principle. Distributing feedback can also be approached as an unsupervised or supervised, possibly interactive, machine learning problem, by leveraging clustering, classification, or mixture modeling methods. Clustering also need not be done by machine learning methods. For example, clone detection methods can identify clones at a variety of levels~\cite{roy2009comparison}.

%Design mining, described in the previous section, is concerned with capturing design choices and then making the space of existing designs explorable. These approaches can be applied to large corpuses of student solutions to the same programming exercise as well. It could be an important tool to assist in the sometimes difficult, manual task of identifying pedagogically valuable examples for illuminating a concept or principle. 

\subsection{Regularizing Code}
Before comparing solutions, it is common to preprocess the code to remove token variability that is irrelevant to later stages in the analysis pipeline. Light preprocessing might include normalizing white space and removing comments. Slightly more preprocessing might include systematically renaming variable and method names, i.e., those chosen by the code author, to generic placeholders. OverCode and Foobaz also preprocess solutions by normalizing whitespace and removing comments. However, since variables are central to the both systems, variables are carefully tracked and strategically renamed, rather than replaced by generic placeholders. OverCode’s normalization is novel in that its design decisions were made to maximize {\it human readability} of the resulting code. As a side-effect, syntactic differences between answers are also reduced.

%e.g., normalizing white space, removing comments, and/or mapping author-chosen variable and method names to generic, unique strings like $var0$ or $str1$. %. , with or without the regularization of variable and method names, whitespace, and comments    with or without the regularization of variable and method names, whitespace, and comments

More aggressive code preprocessing can remove syntactic differences between solutions that are semantically similar by using semantics-preserving transformations. This can include standard compiler optimizations, such as dead code removal, constant folding, copy propagation, and the inlining of helper functions~\cite{rivers2015data}. It can also include transformations, like changes in the order of operands to a commutative function, that make one solution closer to another solution with respect to some definition of edit distance. Applying semantics-preserving transformations, sometimes referred to as normalization or standardization, has been used for a variety of applications, including detecting clones~\cite{baxter,CCFinder}, diagnosis of bugs in student-written programs~\cite{xutransformation}, and self-improving intelligent tutoring systems~\cite{rivers2015data}. 

A schema, in the context of programming, is a high-level cognitive construct by which humans understand or generate code to solve problems~\cite{Soloway1984}. For example, two programs that implement bubble sort have the same schema, bubble sort, even though they may have different low-level implementations. While powerful, these semantics-preserving transformations will not change the schema(s) within a solution. This is not true for the probabilistic semantically equivalent AST subtrees algorithmically mined from a corpus of solutions by Nguyen et al.~\cite{codewebs}. These are probabilistic equivalences because the different AST subtrees are only verified to be semantically equivalent {\it with respect to} the problem and the specific test cases provided. 

%When two pieces of code have different syntax, and therefore different abstract syntax trees (ASTs), they may still be semantically equivalent. A teacher viewing the code may want to see those syntactic differences, or may want to ignore them in order to focus on semantic differences. Semantics-preserving transformations can reduce or eliminate the syntactic differences between code.

\subsection{Features and Distance Functions}

The challenge is designing features or distance functions that capture what the teacher cares about. Teachers may want to give feedback on correctness, design decisions (a.k.a., code style), or both. Other applications of code similarity functions are clone detection and plagiarism. Solutions can be represented as sets or vectors of hand-crafted computed features, sequences of tokens, or graphs. In the context of software, tokens could be characters or tokens corresponding to the lexical rules of the programming language. Drummond et al.~\cite{drummond2014learning} catalogues additional distance measures that are potentially helpful for clustering interactive programs.

\subsubsection{Features}
Just as the authors of Webzeitgeist defined sets of features to be computed for each node in a web page's DOM~\cite{webzeitgeist}, there are many sets of features used to estimate program similarity in the literature. Aggrawal et al.~\cite{srikant2014system}, Elenbogen and Seliya~\cite{Elenbogen}, Roger ~\cite{ACESthesis}, Rees~\cite{Rees:1982}, Huang et al.~\cite{MOOCshop}, Kaleeswaran et al.~\cite{kaleeswaran2016semi}, and Taherkhani et al.~\cite{taherkhani2010recognizing} each defined a set of features that could be computed automatically for each solution and represented as a numerical feature vector. These feature sets each include a mixture of static and/or dynamic features.

Static features include but are not limited to counts of the solution's various keywords and tokens, e.g., control flow keywords, operators, constants, and external function calls; counts of each type of expression in the solution;  measures of code complexity; length of commented code; scores from linting scripts; function name lengths; line lengths; and entire solution lengths. To quantify the goodness of a solution's style, AutoStyle~\cite{choudhury2016autostyle} used a pre-existing metric called the ABC score, a weighted count of assignments, branches, and conditional statements in a block of code.

Dynamic features include but are not limited to data collected from running a solution on each test case, e.g., the solution's output and whether or not it is correct with respect to the teacher's specification, the evolution of values assigned to each variable within the solution, and the order in which statements in the solution are executed. For example, Huang et al.~\cite{MOOCshop} create an output vector for each solution: a binary vector representing the solution's correctness on each test case. For approximately one million solutions to 42 programming exercises Stanford's original Machine Learning MOOC, the authors found that a teacher could cover 90\% of solutions or more in almost all problems by annotating the top 50 output vectors. However, this could be difficult. The authors acknowledge that many different mistakes can produce to the same output vector.

{\bf Role of Variables Theory}

Variable behavior is a specific kind of dynamic feature that merits additional description. Just as there is evidence that experts subconsciously internalize {\it control flow plans}, like the Running Total Loop Plan that accumulates partial totals, there is evidence that experts subconsciously internalize {\it variable plans} ~\cite{variableplans}. Variable plans are characterized by the function or role that it serves in a function, how it is initialized and updated, and conditional statements on the variable's value. 

Empirically, the number of distinct variable roles found in introductory-level programs is small. While reviewing three introductory programming textbooks written for Pascal, Sajaniemi~\cite{sajaniemi2002empirical} hand-labeled the role of variables within each provided example program, based only on the pattern of successive values each variable took on. Nine variable roles, e.g., ``stepper'' and ``one-way flag'', covered 99\% of the variables in all 109 programs found in those textbooks. Sajaniemi notes that a single variable can switch roles during execution, properly or sporadically. A proper switch is when its final value while serving in one role is its initial value when serving in the next role. A sporadic switch is one in which the variable is reset to a new value at some point, to serve a new role that may or may not have anything to do with its previous role. \todo{find and look at green1985programmer--The Programmer's Torch--to summarize how it's different, based on sajaniemi2002empirical's description} Sajaniemi has been credited for creating what is now called in the literature as role of variables theory. 

Further work independently confirms and operationalizes Sajaniemi's insights. Taherkhani et al. ~\cite{taherkhani2010recognizing} found that 11 variable roles cover all variables in novice-level programs, including object-oriented, procedural, and functional programming styles, and went further to automatically classify algorithms based on variables and some additional features. Gulwani et al.~\cite{gulwani_fse14} also depend on variable behavior to recognize different algorithmic approaches. They ask teachers to annotate source code, by hand, with key values that differentiate algorithmic approaches. Gulwani et al.'s work was published at a conference six months after the submission of the OverCode manuscript to TOCHI.

OverCode and Foobaz make use of a pipeline that characterizes each solution as (1) a set of variables, which are distinguished from each other by their dynamic behavior during execution on test cases, (2) a set of one-line statements normalized in a novel way by those identified variables and (3) the solution's outputs in response to each test case. After OverCode's publication, Gulwani et al.~\cite{gulwani2016automated} used the same variable value tracing method to cluster solutions, augmenting it with information about control flow structure to overcome syntactic differences between solutions.

Alternative distance functions based on tokens or solution-derived graphs, like ASTs, are included in the next sections for completeness, and so that the techniques can be revisited in discussions of future work.

\subsubsection{Distance functions}

Statistical natural language processing techniques can also be applied to code, preferably after preprocessing. For example, Biegel et al.~\cite{Biegel} described how $w$-shingling can capture local patterns within a solution. The $w$-shingling of a solution is the set of all unique subsequences of $w$ tokens it contains \cite{BRODER19971157}. The resemblance $r$ between two solutions is defined as the number of unique subsequences of $w$ tokens both solutions contain, normalized (divided) by the union of unique subsequences of $w$ tokens contained in either solution. It other words, it is the Jaccard coefficient of the two solutions' $w$-shinglings. The resemblance distance is defined as $1-r$, which obeys the triangle inequality. $n$-grams models are like $w$-shinglings except instead of capturing just the {\it set} of unique subsequences of a certain length, they also capture a more global feature: the relative frequencies of these unique subsequences in the entire solution or corpus. Similarly, representing Python programs as TF-IDF vectors calculated from counts of tokens, e.g., keywords, collected across the entire corpus of solutions can capture individual solution's deviations from whole-corpus trends~\cite{Gaudencio}.

CCFinder~\cite{CCFinder} and MOSS~\cite{schleimer2003winnowing} (Measure Of Software Similarity) are both pair-wise similarity (clone) detectors. Like $w$-shingling and $n$-gram models, MOSS extracts all subsequences of tokens of a specified length. Unlike them, the order of these subsequences is preserved. CCFinder \cite{CCFinder} is an exception to this pattern. After aggressive pre-processing, it considers all sub-strings in both solutions and looks for matches.

% \subsubsection{Structure-based features and distance functions}

The structure of solutions can be represented as trees, i.e., ASTs, and graphs, e.g., data dependency and control flow graphs. Binary or numerical feature vectors can be computed from the graphs, as well as graph-to-graph metrics of similarity, for use in feedback or assessment~\cite{Robinson:1980,srikant2014system}. Recent literature uses the full AST for computing pairwise distance metrics, e.g., the tree edit distance (TED). TED is defined as the minimum cost sequence of node edits that transforms one AST into another, given some set of costs on types of edits. 

%The tree edit distance (TED) between two ASTs is the minimum cost sequence of node edits that transforms one AST into another, given some set of costs on types of edits. 

While a naive TED algorithm scales very poorly with tree size, an optimized TED algorithm~\cite{shasha1994exact} makes this computation more feasible. The optimized algorithm is only quadratic in both the number of solutions and the size of their ASTs~\cite{MOOCshop}. In contrast, the analysis pipeline used by both OverCode and Foobaz clusters solutions scales linearly with the number and size of solutions. 

Huang et al.~\cite{MOOCshop} used the optimized TED algorithm to process approximately a million solutions while executing on a computing cluster. The same analysis pipeline was used by Roger et al.~\cite{ACESthesis}. Yin et al.~\cite{yin2015clustering} defined a normalized TED that weighs edits associated with nodes closer to the tree root more heavily than nodes closer to the leaves. This prioritizes high-level structural similarities between solutions and de-emphasizes minor differences in syntax near the AST leaves.

\todo{add http://web.stanford.edu/~cpiech/bio/papers/programEncoding.pdf}

%In contrast to these systems that rely on TED for pairwise distance computations, 


\subsection{Clustering Solutions}

Gaudencio et al.~\cite{Gaudencio} recently investigated whether computers might be able to compare student code solutions as well as teachers. In the process, they found that teachers only agreed with each other between 62\% and 95\% of the time. Similarly, Rogers et al.~\cite{ACESthesis} found that official graders for a large programming course at Berkeley agreed on solution grades only 47.5\% of the time even when there were only 3 possible grades to choose from. In spite of the lack of agreement across expert code evaluators, several research efforts have focused on automatically clustering and grading solutions.

%\subsubsection{Clustering}
%In addition to annotating groups of solutions with feedback, 
Luxton-Reilly et al.~\cite{Luxton13} suggests that identifying distinct clusters of solutions can help instructors select appropriate examples of code for helping students learn, e.g., in accordance with the systematic variation suggested by Variation Theory. They also suggest that it is helpful for teachers' own understanding and quality of feedback and guidance. Clustering can also be used to guide rubric creation.

Luxton-Reilly et al.~\cite{Luxton13} develop a hierarchical clustering taxonomy for types of solution variations, from high- to low-level: structural, syntactic, or presentation-related. The structural similarity between solutions in a dataset is captured by comparing their control flow graphs. If the control flow of two solutions is the same, then the syntactic variation within the blocks of code is compared by looking at the sequence of token classes. Variation in presentation, such as variable names and spacing, is only examined when two solutions are structurally and syntactically the same. However, it is not yet fully implemented. %In contrast, our approach is not hierarchical, and uses dynamic information in addition to syntactic information.

The analysis pipeline behind OverCode and Foobaz cluster solutions based on whether or not they have the same output vector and the same set of variables and normalized statements. Other systems rely on clustering algorithms applied to solutions whose pairwise distances are determined by TED scores. AutoStyle~\cite{choudhury2016autostyle} uses the OPTICS clustering algorithm to cluster solutions based on normalized TED scores. That work was inspired by early published OverCode work~\cite{glassman2014feature} on hierarchically clustering solutions. Huang et al.~\cite{MOOCshop} and Rogers et al.~\cite{ACESthesis} cluster solutions by creating a graph where nodes are solutions and an edge between each pair of nodes exists if and only if the TED between their ASTs is below a user-specified threshold. Modularity is used to infer clusters within the graph. 

Gross et al. \cite{gross} use the Relational Neural Gas technique (RNG) to cluster graded submissions and find solutions, called prototypes, that can represent entire clusters. Feedback on new submissions is provided by highlighting the differences between the new submission and the closest prototype. Seeing these differences can help students debug their code. 

Unlike the work in this thesis, Gross et al. focus on providing feedback to a single student at a time. Graded submissions are clustered, and these clusters help identify problems in news submission. In contrast, OverCode, Foobaz, and GroverCode process ungraded submissions and help staff compose feedback for the whole class or assign grades or feedback to a whole body of submissions at one time. OverCode and GroverCode do highlight differences between submissions to help pinpoint problems, although it is staff, rather than students, who view these differences.

Another approach to clustering solutions comes from the field of Bayesian inference. The following methods are particularly relevant to clustering code:
\begin{itemize}
\item Bayesian Case Models (BCM) \citet{beenNIPS} learn a pre-set number of subspace clusters, where each cluster is represented by an example and a small set of features that help characterize that cluster. This representation of the clusters has been designed to increase human interpretability of the results. This model has has an interactive version, iBCM \cite{beenthesis}, where humans can directly modify the cluster example and important features that characterize a cluster.
\item Mind the Gap model (MGM) by \citet{kim2015mind} clusters data while also learning a ``global set of distinguishable dimensions to assist with further data exploration and hypothesis generation.'' This is another interpretable clustering algorithm, which may explain itself in ways teachers can understand and base grades on with confidence.
\item Dirichlet Process Mixture Models (DPMMs)~\cite{} do not require the number of clusters to be set beforehand. The number of clusters solutions are assigned to can grow as the number of solutions grow. However, every data point belongs to one cluster.
\end{itemize}
\todo{get DPMM citation}

%into an intelligent tutoring system that provides a variety of feedback messages coaching students toward solutions with better coding style, based on an automatically computable heuristic for good code style.\todo{expand on it}

\todo{Maybe add Classification for grading: Aggrawal et al. [successful] Rogers [unsuccessful] Taherkhani [successful]}

%Drummond et al.~\cite{drummond2014learning} use Bayesian techniques to cluster solutions into one of two categories: ''good'' and ''bad''.

\todo{add http://ethanfast.com/resources/deduceit-paper.pdf}

%\todo{add bayesian clustering--like BCM}

\todo{add https://arxiv.org/pdf/1501.04346v1.pdf}

\subsection{Identifying Common Components Across Solutions}

Since teacher's holistic grades can be so inconsistent, they may be internally relying on different rubrics, not equally sensitive to minor differences, and/or weighing factors differently. Teachers may be more consistent if, rather than generating holistic grades, they can annotate or grade particular components, mistakes, or design choices within solutions. Three existing approaches support this goal: (1) Create a classifier for every component, mistake, or design choice teachers are interested in, similar to what was done for web pages in the Webzeitgeist dataset by Lim et al.~\cite{lim2012learning}. (2) Model solutions as mixtures of components or design choices using mixture modeling. They have already been applied to source code~\cite{} and student solutions to open-response mathematical questions~\cite{} ~\cite{binkley2014understanding,Linstead}. (3) Create a code search engine which takes AST nodes or subtrees as queries and retrieves solutions in the database that contain them, as Nguyen et al.~\cite{codewebs} did for solutions to the same problem and Fast et al.~\cite{codex} did for general open source code.

There is a particularly rich literature on Bayesian mixture models. The following methods are particularly relevant to modeling code as mixtures of choices:
\begin{itemize}
\item Latent Dirichlet Allocation (LDA)~\cite{bleiLDA} is a mixture model that is typically applied to natural language. In that context, it learns topics, i.e., distributions over words, as well as the distributions over topics found in each document. %, which is typically applied to corpuses of documents could model solutions as "documents" with sets of "words," where each word belongs to one topic and each document can contain words from multiple topics.
\item Correlated Topic Models (CTM)~\cite{} are like LDA but topics are no longer assumed to be independent. In other words, the topics learned by the model can be correlated. If it it possible to formulate the inputs such that the learned latent topics represent design decisions, this could capture the reality that design choices are not independent of each other in code composition.
\item Hierarchical Dirichlet Process Models (HDPs)~\cite{}, like DPMMs, do not require a pre-set number of clusters. Unlike DPMMs, solutions do not belong to a single cluster. Like LDA, solutions contain features, and each feature belongs to a cluster. Solutions can contain features from multiple clusters. This method emulates LDA with no pre-set number of clusters. 
\item Models and inference algorithms built on Indian Buffet Processes (IBPs) by \citet{doshi2009indian} are like HDPs, but individual features can belong to multiple clusters.
\end{itemize}

\todo{get CTM and HDP citations}

\subsection{Visualization and Interfaces}

There are several existing visualizations or interfaces that help users understand how solutions vary within a large corpus of solutions to a common problem. A common design choice is to map each solution to a point in some feature space. Huang et al.~\cite{MOOCshop}, Rogers et al.~\cite{ACESthesis}, AutoStyle~\cite{choudhury2016autostyle}, and Ned Gulley~\cite{ICERGlassman} all use this strategy. % and/or why a given set of solutions are grouped together

%Cody\footnote{\url{mathworks.com/matlabcentral/cody}} is an informal learning environment for the Matlab programming language that presents learners with {\em solution maps}. 

Ned Gulley designed solutions maps for Cody, a Matlab programming game\footnote{\url{mathworks.com/matlabcentral/cody}}, to help users pick and compare pairs of solutions from hundreds of solutions to the same programming exercise. The solution map plots each solution as a point against two axes: time of submission on the horizontal axis, and code size on the vertical axis, where code size is the number of nodes in the parse tree of the solution. Users can select pairs of points to see the code they correspond to side-by-side beneath the solution map. Despite the simplicity of this metric, solution maps can provide quick and valuable insight when exploring differences among large numbers of solutions~\cite{ICERGlassman}. This can help game players learn alternative, possibly better, ways to solve a problem using the Matlab programming language, including its extensive libraries.

Huang et al.~\cite{MOOCshop} and Rogers et al.~\cite{ACESthesis} create graphs where each node is a solution and links between nodes indicate similarity scores beneath a certain threshold. Inter-node and inter-cluster distances correspond to syntactic similarity. Clusters are colored using modularity, a measure of how well a network decomposes into modular communities. Rogers et al.~\cite{ACESthesis} built a grading interface on top of this clustering process. Graders graded one solution at a time, grouped by cluster.

AutoStyle~\cite{choudhury2016autostyle} visualize all solutions on the screen using a t-SNE 2D visualization. Similar to the previous clustering interfaces, each point represents a solution and its color indicates its cluster. Hovering over a point reveals the solution it represents. Using this interface, teachers hand-annotate each cluster with a label, i.e., ``good'', ``average'', or ``weak'' and a hint about how to improve the solution. For each cluster, the teacher must also choose an exemplar solution from a {\it slightly better} cluster as an example of what to shoot for.

Tools that are not built for representing an entire corpus, such as file comparison tools, do have useful features to consider when designing new interfaces. Most highlight inserted, deleted, and changed text. Unchanged text is often collapsed. Some of these tools are customized for analyzing code, such as Code Compare. They are also integrated into existing integrated development environments (IDE), including IntelliJ IDEA and Eclipse. These code-specific comparison tools may match methods rather than just comparing lines. Three panes side-by-side are used to show code during three-way merges of file differences. There are tools, e.g., KDiff3, which will show the differences between four files when performing a distributed version control merge operation, but that appears to be an upper limit. These tools do not scale beyond comparing a handful of programs simultaneously.

The OverCode interface puts the code front and center, synthesizing code examples that represent entire stacks of solutions, borrowing display techniques from file comparison tools, adding filtering mechanisms and interactive clustering through rewrite rules on top of the clustering done by the analysis pipeline. OverCode was also inspired by information visualization projects like WordSeer \cite{wordseerlitcomp13,wordseercikm13} and CrowdScape \cite{crowdscape}. WordSeer helps literary analysts navigate and explore texts, using query words and phrases \cite{wordseerhcir11}. CrowdScape gives users an overview of crowd-workers’ performance on tasks.

More generally, several interfaces have been designed for providing grades or feedback to students at scale, and for browsing large collections in general, not just student solutions. The powergrading paradigm \cite{basupowergrading} enables teachers to assign grades or write feedback to many similar answers at once. Their interface focused on powergrading for short-answer questions from the U.S. Citizenship exam. After machine learning clustered answers, the frontend allowed teachers to read, grade, or provide feedback on similar answers simultaneously. When compared against a baseline interface, the teachers assigned grades to students substantially faster, gave more feedback to students, and developed a ``high-level view of students' understanding and misconceptions'' \cite{basuDivideAndConquer}.



%\subsection{Handling Incorrect Solutions}

%\todo{summarize Singh's Autograder, evan pu's paper?}
\section{Personalized Support}
Several types of solutions have been deployed to help students get the personalized attention they need. These solutions span the spectrum from recruiting more teaching assistants from the ranks of previous students \cite{communityTAs} to automating hints using program synthesis or intelligent tutoring systems. 

Singh et al. \cite{rishabh} uses a constraint-based synthesis algorithm to find the minimal changes needed to make an incorrect Python solution functionally equivalent to a reference implementation. The changes are specified in terms of a problem-specific error model that captures the common mistakes students make on a particular problem. The system can automatically deliver hints to students about these changes at various levels of specificity. 

%\subsection{Intelligent Tutoring Systems}
Intelligent tutoring systems can provide personalized hints and other assistance to each student based on a pre-programmed student model. For example, previous systems sought to provide support through the use of adaptive scripts \cite{kumar2007tutorial}, or cues from the student’s problem-solving actions \cite{diziol}. Despite the advantage of automated support, intelligent tutoring systems often require domain experts to design and build them, making them expensive to develop. \todo{Include ``current ITSs require an exact formalization of the underlying domain knowledge
which is usually a substantial amount of work: researchers have reported 100-1000 hours
of authoring time needed for one hour of instruction [MBA03] from `Feedback Provision Strategies in Intelligent Tutoring
Systems Based on Clustered Solution Spaces'''} Furthermore, domain experts who generate these hints may also suffer from the {\it curse of knowledge}: the difficulty experts have when trying to see something from a novice's point of view \cite{curse}. 

%\subsection{Through Data Mining}
Unlike intelligent tutoring systems, the HelpMeOut system \cite{helpmeout} does not require a pre-programmed student model. It assists programmers during their debugging by suggesting code modifications mined from debugging performed by previous programmers. However, the suggestions lack explanations in plain language unless they are added by experts (teachers), so the limits imposed by the time, expense, and curse of knowledge of experts still apply.

Rivers and Koedinger \cite{riversaied} propose a data-driven approach to create a solution space consisting of all possible paths from the problem statement to a correct solution. To project code onto this solution space, the authors apply a set of normalizing program transformations to simplify, anonymize, and order the program's syntax. The solution space can then be used to locate the potential learning progression for a student submission and provide hints on how to correct their attempt. %Unlike OverCode’s variable renaming method, which reflects the most common names chosen by students, Rivers and Koedinger replace student variable names with arbitrary symbols, i.e. \codevar{daysInMonth} might be mapped to \codevar{v0}. 

%\subsection{Learnersourcing and Peer Instruction}
Discussion forums derive their value from the content produced by the teachers and students who use them. These systems can harness the benefits of peer learning, where students can benefit from generating and receiving help from each other. However, as the system has no student model, the information is available to all students whether or not it is ultimately relevant. Students can receive personalized attention only if they post a question and receive a response. 

Peer-pairing can stand in place of staff assistance, to both reduce the load on teaching staff and give students a chance to gain ownership of material through teaching it to someone else. Weld et al. speculate about peer-pairing in MOOCs based on student competency measures \cite{WeldHcomp12}, and Klemmer et al. demonstrate peer assessments' scalability to large online design-oriented classes~\cite{Klemmer}. Peer instruction~\cite{mazur} and peer assessment~\cite{peerassessment} have been integrated into many classroom activities and have also formed the basis of several online systems for peer-learning. For example, Talkabout organizes students into discussion groups based on characteristics such as gender or geographic balance \cite{talkabout}.

Recent work on learnersourcing proposes that learners can collectively generate educational content for future learners while engaging in a meaningful learning experience themselves \cite{kim2013learnersourcing,weir2015,mitros2015}. For example, Crowdy enables people to annotate how-to videos while simultaneously learning from the video \cite{weir2015}. The targeted learnersourcing workflows presented in this thesis expand on learnersourcing by requesting the contributions of specific learners who, by virtue of their work so far, are uniquely situated to compose a hint for fellow learners. %I will refer to this as targeted learnersourcing.

\todo{re add some form of this sentence: Therefore, peer groups, home environment, learning communities, and identity formation~\cite{walberg1984improving,case2008education} are beyond our consideration.}

\todo{consider adding additional papers from notes}

\todo{add http://research.microsoft.com/en-us/um/people/sumitg/pubs/fse14.pdf ''Our key insight is that the algorithmic strategy employed
by a program can be identi ed by observing the values com-
puted during the execution of the program.''}

\begin{comment}
\section{Augmented Intelligence}
     "The power of the unaided mind is highly overrated… The real powers come from devising external aids that enhance cognitive abilities. " —Donald Norman 
     http://www.dougengelbart.org/pubs/augment-3906.html
\end{comment}