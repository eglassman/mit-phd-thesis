\chapter{Discussion}\label{chapter:discussion}

The systems in this thesis give teachers more awareness about the content generated by students in large programming classes and enable styles of teaching that are usually only feasible in smaller classes, such as discussions of variation and style that are directly driven by what the students have already written. These systems also to scale up automated compare-contrast, self-explanation, and formative assessment-style exercises whose content is generated by students and curated by teachers. \todo{add citations for value of formative assessment to related work} The current state of the art in theories of how humans learn predict that these supported interactions between teachers and students will enhance learning.

%the recognized value of strategic variation within examples presented to students. 

%\todo{Find a place for this: OverCode is designed for thousands of correct solutions to introductory Python programming problems. The focus began with correct solutions because many residential and online exercises now afford automatic, immediate feedback about the correctness of a solution. Given that feedback, nearly all the final student solutions are correct, but it is clear from reading through them that some students have better command of programming than others, or even underlying misconceptions revealed in their correct code, just based on compositional quality. }

%\todo{make sure I revisit or at least cover my thesis statement: "Clustering and visualizing solution variation collected from programming courses can help teachers gain insights into student design choices, detect autograder failures, award partial credit, use targeted learnersourcing to collect hints for other students, and give personalized style feedback at scale."}

\section{Design Decisions and Choices}

%As the complexity of code increases, students face more design decisions.

This thesis work began with the vision that a teacher would someday be able to look at a display summarizing hundreds or thousands of student solutions to the same problem and immediately see---and comment on---good and bad design decisions that students made. The number of possible distinct student solutions grows rapidly with the number of design decisions and design choices students can make. 

The solution space could be imagined as a large n-dimensional space where each solution has a single coordinate. Each design decision, e.g., whether to solve a problem iteratively or recursively, would be a dimension in the solution space. The choices students make could be thought of as discrete points along that dimension in solution space. While the number of distinct combinations of design choices students choose can be large, the number of dimensions in this space, i.e., the underlying design decisions, grows much more slowly. 

However, the structure of this hypothetical solution space ignores how design choices affect each other. Some design decisions are mutually exclusive, e.g., looping over a particular array with a for or a while, some decisions are correlated with one another, and some decisions are completely independent. A tree-like description of the solution space can capture some of these relationships between decisions. If a node represents a design choice, e.g., to loop over an array, there may be two or more choices, e.g., a \texttt{for} or a \texttt{while} loop, that can be represented as child nodes. The choice to use a \texttt{for} loop poses an additional design design, e.g., whether to use \texttt{range} or \texttt{xrange}: \texttt{for i in [x]range(input)}. The average depth of the tree would correspond to the average number of design decisions the students faced and the average branching factor would correspond to the average number of distinct choices students in the corpus make at each decision point.

The curse of dimensionality predicts that, as we add more and more dimensions to the solution space, the density of solutions will decrease and the liklihood that any two solutions occupy the same location in that space will go down. The regularity of code discussed in the chapter on related work should help ameliorate the curse of dimensionality, but only partly. In other domains, it is often necessary to collect more data as the dimensionality of the space increases. However, given that the solutions are generated by students, the number of correct solutions are more likely to go down rather than up as the complexity of solutions, and the associated dimensionality of the solution space, increases.

This difficulties posed by high-dimensional spaces have been addressed, in each problem tackled, by choosing what information to ignore and, in some cases, what information to index by. For example, OverCode ignores white space, comments, variable names, and statement orders. It indexes by canonicalized lines. In order to assign a variable name quiz, Foobaz looks at the behavior of the variables in the student's solution, ignoring everything else about the solution's composition. Dear Beta and GroverCode forget what values cause a solution to fail a test case, preserving only that the test case has failed. Dear Gamma ignores circuit topology and indexes by the number of transistors.

It is important to note that this work is not intended to capture an enumeration of all possible design choices and resulting solutions. It is intended to capture the design choices students are actually making. The relative popularity of these choices is discussed in the section that follows.

\section{Capturing the Underlying Distribution}

There will be some design decisions within each solution that are rare and some that are common, some that exemplify good programming practices and others that do not. These decisions might create inefficient solutions, reveal a student's fundamental misunderstanding, or use a feature of the language in a creative way. The distribution of solutions along these dimensions of variation may reflect student prior knowledge, teacher explanations, and common misconceptions. 

During Hannah Wallach's invited talk at the interpretable machine learning workshop at ICML 2016, she made the following observation: computer scientists are often looking for better ways to find needles in haystacks and computationally-minded social scientists are trying to characterize the haystack. One could think of work like Codex~\cite{codex} and Webzeitgeist~\cite{webzeitgeist} as mechanisms for finding needles in haystacks through creative indexing of Ruby code and webpages, respectively. OverCode, Foobaz, Dear Beta, Dear Gamma, and GroverCode are trying to faithfully represent the haystack itself, while also supporting needle-finding. 

\section{Writing Solutions Well}

The focus on introductory Python programming courses is often just correctness. The MIT EECS introductory Python programming course whose staff used GroverCode makes it a policy not to penalize students on how their solution is written. This means that they sometimes have to hand out full marks to a solution that makes them groan.

This policy may exist for several reasons. The first is the clarity of the policy: if it is correct, it gets full credit. Second is the clarity of the evaluation: if it passes the suite of test cases, it is correct. Third is the apparent appropriateness of the objective for novices: it may be too hard for novices to write a solution well in addition to achieving correctness. % worry about writing well in addition to writing correctly.% bear both the cognitive load of writing a solution well in addition to correctly.

However, it can also be very hard to achieve a correct solution if it is not written well. Something as simple as poor variable names, such as giving an array index a name that suggests it holds the value of the array at that index, can cause students to produce incorrect code.

Just as there is no silver bullet for writing prose well, there is no silver bullet for writing solutions well. Solutions, prose, products, buildings etc., are all designed, and each community has its own ways for how to help students make good design decisions. For example, in Steven Pinker's book "The Sense of Style", he suggests and then demonstrates how to pick apart examples of good writing to understand what makes them good. Students of a particular design form may attend design studios, where they discuss each of their work in turn and give and receive constructive criticism from peers. In both these examples of prose and design, the designed objects are examined individually and also as a group, emulating the conditions for learning from variation espoused by the theories of learning reviewed in the related work. 

Perhaps because software can be marvelously complex, there is less of a design studio culture for software. Many software companies compose and maintain prescriptive style guides against which new code is carefully compared. These guides are not necessarily built on data about how software engineers actually design their code. 

While peer review is not a design studio, they are closer to that mode of education. Peer review practices are now being used in large online courses in order to make up for small teacher-to-student ratios. Some residential software engineering courses, e.g., MIT's 6.005, also set up infrastructure for peer review, even when the number of staff members is sufficient. This forces students to engage with some (usually random) sampling of how other students solved a problem. 

However, the work in this thesis takes this idea farther. Rather than hope that the randomly assigned peer review experiences provides students with a sufficient variety of examples, the work in this thesis attempts to pull out the design dimensions as well as concrete examples along those dimensions and, when possible, ask students to engage with them in a targeted, personalized way. 

This may be helpful even at the level of introductory programming. For example, according to Variation Theory, a student will understand the concept of a loop in a more generalized, robust way if they have seen all the different ways in which their peers have written that loop. The teacher can quickly and easily provide an expert's perspective by commenting on the popular and rare choices. Students can be overwhelmed by choices, e.g., "Should I use \texttt{range} or \texttt{xrange}? Does it matter?" With concrete examples, teachers can help students identify what matters and what does not. 

Many students now taking introductory programming courses will take these skills with them to other majors. While computer science majors can acquire the skills of writing code well in more advanced software engineering classes, the lessons in introductory courses on writing code well may be the only ones non-majors ever get.

%Many software companies put each commit to the company code respository through a thorough code review after it has passed all the required test suites. Rather than looking at other solutions written by , they may rely heavily on carefully composed prescriptive style guides.

%In order to write well, 

%Software is designed. 

%because that is a large hurdle in of itself, especially for novices. This is reinforced by the ease of automatically checking a student solution against a suite of test cases, compared to automating the analysis of its inner design. However, even at the most basic levels, presenting students with examples that vary in particular ways

%Teaching People How to Write Well

%Enabling Design Studio-style Instruction

%This approach is intended to support both a design studio style of instruction, e.g., discussions of design trade-offs while looking at the space of solutions, as well as help teachers pluck pedagogically valuable examples from the distribution.% that support better learning outcomes.

%semi- or fully-automatically prompting students to engage with the types of solution variation predicted by theories of learning to support better learning outcomes.


%Foobaz demonstrated how this could be done, on a smaller scale, just in the context of variable names. It is not unreasonable to assume that it can also be achieved at the level of entire programs. 

%Hundreds or thousands of solutions to the same introductory python programming prompt can share a lot of structure and behavior. Empirically, the number of different design decisions made by students can be relatively small, or at least distributed in such a way that the majority of students' decisions fall into a small number of categories, with the remaining students making much more unique design choices. 

%The OverCode pipeline reduces the number of distinct solutions by collapsing dimensions that exist only because of differences in variable names or behaviorally equivalent statement orders. The pipeline helps reduce the number of student solutions a teacher needs to consider, but it does not fulfill the original vision: turning thousands of solutions into a small set of solution exemplars or facets of solutions that exemplify the common underlying student design decisions. 

%While the number of distinct combinations of design choices students choose can be large, the number of underlying design decisions grows much more slowly. %This chapter explores how latent variable models can capture and communicate design decisions within the solutions. 

 %In other words, the systems in this thesis are in large part an interest in exposing, for commentary and reflection, what students actually write. %, not what the, not what they "should" do.% actually produce, not what they should produce.% what students "should" do, but what they

%Finding the Needle vs. Characterizing the Haystack: Codex, etc. is needle finding; my methods characterize the haystack
%\todo{read "to explain or to predict" paper, add design studio education to related work}

\section{Clustering and Visualizing Solution Variation}

OverCode is a form of unsupervised clustering. Clustering is a function of similiarity measures and mechanisms for grouping or splitting clusters of data points. There is no true correct answer, but there are distinct failure modes when using it in the context of teachers reviewing solutions. Two are most relevant in this work. First, the representation of the solution and/or the measure of similiarity between solutions can ignore, hide, or otherwise fail to capture what the teacher cares about. Second, when the teacher cannot infer what a cluster "means" based on its members and the clustering algorithm cannot explicitly communicate why the cluster exists, the teacher may not trust the clustering and may not feel comfortable using it for propagating feedback and grades back to students. This is exacerbated when the teacher discovers a member of a cluster that seems not to belong.

The OverCode clustering pipeline attempts to escape the first clustering failure mode: erasing distinctions that the teacher may care about. For reasons discussed in detail in the OverCode chapter, OverCode is designed to reveal to the teacher what their students' solutions actually look like, modulo white space and comments. These solutions are rendered for the teacher using the most common variable names and statement orders.\todo{check the statement order} To stay true to what students actually wrote, this process preserves syntactic differences. For example, when iteratively exponentiating \texttt{base} to an exponent, there are multiple ways to multiply an accummulating variable \texttt{result} by \texttt{base} and save the product back into \texttt{result}, such as \texttt{result *= base} and \texttt{result = result * base}. If the teacher just gave a lecture on common forms of syntactic sugar, OverCode will be sensitive to whether or not students use it. OverCode would allow teachers to observe whether students absorbed the lesson on syntactic sugar based on the way they write their subsequent solutions. The fact that even small differences in syntax creates separate clusters within the OverCode pipeline nearly ensures that any syntactic choices a teacher is interested in has been preserved and can be filtered for in the interface.

 %ignored during the OverCode pipeline %decide for themselves what they want to ignore and 

%\section{Visualization}

The second clustering failure mode--producing clusters that the teacher does not trust--is avoided in several complementary ways. First, their is a clear interpretation of what an OverCode cluster can and cannot include, based on the canonicalized solution that represents it. Specifically, all solutions in the cluster have the same set of lines after the behavior-based canonicalization of their variable names, regularization of white space, and removal of comments. Second, the differences between clusters is made clear by highlighting which lines make the non-reference clusters different from the reference cluster. Third, OverCode's filtering features and rewrite rules help teachers change their view of solutions into one that preserves the differences they are explicitly interested in and ignores those they are not interested in. Note that filtering by semantic choices may only be possible when the work on Bayesian modeling of these solutions becomes more mature. 

\todo{add "Users do not notice renamed variables unless the names are inappropriate or do not look like they are human generated. Variable names as corpus-wide unique identifiers of a particular variable behavior is externally inconsistent and has low learnability."}

In general, the interfaces in this thesis cluster complex objects and visualize those clusters so that there is little guesswork about what is included and excluded in a group and what the boundary between groups is. There are no outliers that are grouped with something else without explanation. Rather than losing faith in the clustering process, outliers can be used as the teacher sees fit to spur improvements either to the software infrastructure of the course, e.g., the input-output testing harness, or the examples students are asked to engage with. 


%\section{Processing Code}

%Just as statistical natural language processing (NLP) techniques rose to prominence alongside more traditional linguist rule-based NLP techniques when more data was available, the literature on processing large corpuses of code reflects both rule-based and statistics-based techniques. The systems in this thesis are both statistical and based on  rule-based, and, as explained in the future work section of the next chapter, are not incompatible with several other 

%rules e.g. compilers vs. statistical methods
%Much of the contemporary work on processing student solutions is 

\section{Language Agnosticism}

It is not surprising that the evolving values of variables would carry significant semantic meaning in code written by students at the introductory level in languages like Python and Matlab, especially if the style of programming is procedural. This thesis confirms that within the context of introductory procedural Python programs. According to Taherkhani et al.~\cite{taherkhani2010recognizing}, variables carry useful semantic meaning in object-oriented and functional programming styles as well. Taherkhani et al.~\cite{}, Gulwani et al.~\cite{gulwani_fse14}, and ~\cite{sajaniemi2002empirical} have all authored variable-behavior-based semantic analyses of Java, C++, and Pascal programming languages, respectively. One could think of it as behavior-based semantic variable duck typing. 

OverCode and Foobaz could also be applied to other more programming languages. For a language to be displayable in OverCode, one would need (1) a logger of variable values inside tested functions, (2) a variable renamer and (3) a formatting standardization script. For non-variable-centric languages like Haskell, other dynamic characteristics of execution would likely need to be tracked.

\section{Limitations}

The thesis presents a series of case studies about how to present the variety of programming solutions in a human-interpretable way and make use of it in pedagogically valuable, scalable ways. The methods described only work in a particular domain of solutions: those that are executable and solve the same programming exercise. This excludes natural language, for example. These case studies embody the design principles espoused, but there is no validated unifying recipe by which a corpus of student solutions can be processed and used. Each corpus and set of teachers' values were considered together in order to engineer a representation of solutions--both in the pipeline and in the user interface--that would empower teachers and benefit students. There are many specific technical limitations of the approaches described in the previous chapters. In the next chapter, the section on future work describes these limitations and suggests next steps.

I am not aware of any complex domain where this kind of feature engineering and design has been automated. General guidelines and trial-and-error are accepted practice. Before the resurgence of deep neural nets training on massive data sets, one could reasonable argue that carefully human-designed features are were responsible for a great deal of the performance of machine learning systems. Otherwise, systems fall into the failure mode of garbage in, garbage out.

\section{Design Recommendations}

While this thesis does not offer a unifying recipe, these are some design recommendations based on experience accumulated since the start of this thesis:
\begin{enumerate}
\item When possible to do with high confidence, propagate human-assigned labels to similar data points.% automatically by the interface, such as grading a single solution and finding out that now an additional 39 solutions will appropriately receive the same grade and feedback.
\item Avoid outliers within clusters at all costs; they cause doubt and confusion. 
\end{enumerate}
%Just as successful deployments of machine learning often depend a great deal on encoding domain knowledge into the design of the features This work does not present any formal process by which to apply similar methods to new programming classrooms or domains. 





\begin{comment}
\section{Human Evaluator Consistency Issues}

%\todo{add  and text from kim and glassman tech report and perspective from icml talk}

Clustering python solutions has a strong subjective component. In early pilot studies, python programming teachers given the same set of solutions partitioned the space of solutions in a variety of ways, producing different numbers and/or compositions of clusters. \todo{add glassman at scale poster info and citation} Since teachers found multiple reasonable clusterings, the pilot results can be explained by one or both of the following reasons: (1) Teachers have different internal clustering metrics. (2) Given a clustering metric, solutions can still belong to multiple clusters.

It is also reasonable to assume that, given additional solutions to the same programming problem, some solutions would fall into existing clusters and some would represent wholly new solution successes or failures. It is reasonable to assume there is not some true total number of clusters that one reaches if one sees enough data.
\end{comment}


%that supports filtering, comparison between clusters, and the addition of problem-specific equivalence rules that allow  may merge clusters .

%--clustering without human-interpretable explanations or with outliers mixed in that surprise the teacher--



%When presented to an MIT lecturer for the residential and online versions of the class OverCode was built to handle, she did not react to the 


%It uses canonicalization and equivalence rules to determine which solutions should be in the same cluster.

%Clustering fails to be useful when the similarity measure or mechanism for grouping or spliting solutions ignores or fails to capture what the teacher cares about.

%This clustering process intentionally preserves low-level differences;

%The  so that it is flexible enough to accommodate a 

%the needs of the teacher are not fully known. One could think of this base level of clustering as the most minimal possible. The OverCode user interface picks up where the clustering pipeline 

%Efforts to cluster complex objects like Python programs can fail to be useful when the metric fails to capture some aspect of the data points  


%One could think of OverCode as the lowest possible level of unsupervised clustering that could reasonably be performed on the solutions in our dataset. 


%The canonicalization process embeds information in each line The equivalence rules recognize  stop short of 

%OverCode performs unsupervised clustering using equivalence rules  based on the  could be thought of as unsupervised clustering with a binary similarity metric.

%\section{Representing Solutions}




%\section{Unsupervised Clustering: No One Right Answer}
%\section{Grouping and Filtering}
%A second major challenge of this work is designing 

%\section{Rendering}
%While the order of arguments to a commutative operation 

%Interpretability







%trusting clusters, saving cognitive load on human viewers
%One of the key principles 


%\section{Theories of Learning}

%\section{Design Mining}

%One could think of OverCode and Foobaz's reliance on variable behavior signatures to establish semantic equivalence as a type of semantic duck typing that is an alternative to the two other recently published variable-behavior-based semantic analyses by Taherkhani et al.~\cite{} and Gulwani et al.~\cite{gulwani_fse14}.%, and ~\cite{sajaniemi2002empirical}, which have been used for the Java, C++, and Pascal programming languages, respectively. 
\begin{comment}
\section{Objectives}

-compose for the benefit of others and yourself
-productive confusion
-feedback as if you had a small classroom
\subsection{teacher insight}
\subsection{Readable code}
Writing readable code is an important part of learning how to code. especially since many students now enrolling in introductory programming courses will not go on to become computer scientists. Students syntactic choices are preserved
Since most solutions are correct with respect to the teacher-provided test cases. The input-output behavior was no longer so informative; the quality of the written solution was still very revealing. Typically, when the solution passes all test cases, there is no more feedback, unless an additional peer evaluation system is set up.

\section{Limitations}

There are many limitations to the current work. 


What about non-primitive data-types?

\end{comment}



%While the regularity of code that was discussed in the chapter on related work does have some protective effect from the curse of dimensionality, it is still the case that, without an increase in the number of solutions, an increase in code complexity  

%and the number of design decisions that comes with it will  unless the number of correct solutions submitted by students also increases, the stacks that represent each combination of design choices become smaller. 

%One of the first such poster abstracts 

%I wrote an \todo{describe learning at scale poster that inspired berkeley stuff}

%OverCode was built for the final correct solutions generated by students in large introductory Python programming courses. Typically, in these courses, thousands of students test their solutions against a teacher-designed test suite until it is correct. The only feedback they get along the way is which test cases their solution fails. There is much more feedback that can be given, and OverCode was designed to empower teachers to give that feedback, informed by the distribution of semantic and syntactic choices made across all student solutions.

%Foobaz demonstrated another way to leverage the distribution of student choices in order to provide 

\begin{comment}
\section{Commonality and Variability with Respect to What?}

One of the main challenges of this work is designing an appropriate similarity measure, one that captures what the users--teachers, students, or both--care about and ignores or hides everything else. As discussed in the chapter on related work, there are a variety of ways to quantify the similarity between two solutions.

In order to be sensitive to the very basic syntactic choices that introductory Python programming students are learning to make, the OverCode 
\end{comment}

%Once a collection of solutions to the same problem have been processed by the OverCode pipeline, there are a variety of features that can be used to describe each solution, including but not limited to canonicalized variables and lines. 

%The OverCode pipeline produces many sets of features for each solution, such as the behavior of variables, the syntax of each line, and the canonicalized lines that encode both behavioral and syntactic information in a single string. However, not all sets of features need to be used, depending on  %It can be useful to throw away some of these features, such as those that encode syntactic information, when focusing on %Choosing subsets of these features, such as just the variable behavior features, can cut down on the dimensionality of the solution space. % because syntactic differences are no longer explicitly captured.

%-accurate cluster naming
%-differences between clusters
%-filtering for discovery rather than aggregation
%-rewrite rules for basic aggregation

%by building a user interface on top of the OverCode clustering pipeline that allows teacher to see all the syntactic and algorithmic variety and gives them tools, i.e., filtering and rewrite rules, to focus on what matters to them. It is designed for exploring student solutions, not generating a small number of coherent clusters.

%\section{Computer Science vs. Social Science}



%\section{Semantic Duck Typing}

