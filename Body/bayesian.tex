\chapter{Statistical Modeling of Solutions}\label{chapter:bayesian}







\section{LDA}


\subsection{Features} 

%Fortunately, programs are also executable. Features from dynamic analysis can complement static token-level features, as was demonstrated in \cite{glassmankimtechreport}. Of particular interest in this work are the features produced by the OverCode program analysis method \cite{glassmantochi}.

%{\bf Common Variables} The OverCode \cite{glassmantochi} analysis pipeline was initially developed to deduplicating Python programs that differ only by variable names, statement order, formatting, and comments. In the process of this deduplication process, 



%\subsection{Model Choice}









%\begin{figure}[ht]
%%\vskip 0.2in
%%\begin{center}
%\includegraphics[width=0.5\columnwidth]{lda_plate_model}
%\caption{Plate model for LDA}
%\label{lda_plate_model}
%%\end{center}
%%\vskip -0.2in
%\end{figure} 



%If we ignored the interdependence of decision choices on each other,  %One can think of this as a tree-structured solution space, where the number of choices that can be made at each decision point

%\section{Feature Choice}

 % rather do not necessarily even stay constant as solution complexity goes up; it may decrease because fewer students are able to compose a correct solution.
%For every additional dimension, one may need many more solutions in order keep the same level of density.





% well suited to summarizing the major design decisions


%Given the relatively small set of underlying design decisions, the common structures shared by many student solutions, and the availability of feature vectors already computed by the OverCode pipeline, appropriately chosen latent variable models may be well suited to summarizing the major design decisions. %and earlier promising experimentation with Bayesian non-parametric methods as part of a collaboration with \citet{beenthesis}, Bayesian non-parametric methods may be key to fulfilling the original vision for OverCode.



%\section{Empirical Qualities of Solution Clusters}





%\section{Model Choice}





\begin{comment}
\section{Plan for Applying Models to Solution Collections}

\begin{enumerate}

\item I implemented a DP-stick-breaking/GEM distribution generator.
\item I studied and understood the stick-breaking construction of the Hierarchical Dirichlet Process found in Section 4.1 of the article Hierarchical Dirichlet Processes by \citet{hdp05}.
\item By April 22nd, I will implement and test, in python, either (1) the Gibbs sampler for CRPs implemented in R and demonstrated in the third MLSS '15 lecture on Bayesian nonparametric statistics or (2) the second algorithm in \citet{neal2000markov}, which performs Gibbs sampling on DPMMs with conjugate priors.
\item By May 4th, I will then generalize the same implementation from the previous step to add a level of hierarchy, in order to perform clustering by Hierarchical Dirichlet Processes. 
\item By May 10th, I will write up the algorithm and its performance on python solutions.
\end{enumerate}

I do have stretch goals, as incentives to finish more quickly, specifically: (1) adding the subspace-learning feature of BCMs to my implementation and (2) predicting rubric labels teaching assistants will apply, based on the labels actually applied by teaching assistants during grading with OverCode.
\end{comment}

%\section{Predicting Rubric Labels}


